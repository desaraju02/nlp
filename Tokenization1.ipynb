{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCFImaq6PsbP1eXHfUaD1v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desaraju02/nlp/blob/main/Tokenization1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRsL42FQfkfp",
        "outputId": "911cd2ec-5344-4452-ed37-685a92c027f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "Welcome to NLP learning\n",
        "We have a lot of ground to cover\n",
        "Years of work\n",
        "Let's catch up in a few months\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "v9EGd7ISfwT9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "jL_HLNV8gCAG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54bf0e1f",
        "outputId": "baa763d1-dd69-4e21-cd10-51aced05e119"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b4651a1",
        "outputId": "36631ea0-b85e-4610-80e6-cabc98af001d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "q96BJwSfgbYG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C-O5qnTgslt",
        "outputId": "56e6f6dd-d146-448f-a9db-ca0db7ba96b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"\\nWelcome to NLP learning\\nWe have a lot of ground to cover\\nYears of work\\nLet's catch up in a few months\"]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM2rb6UngukM",
        "outputId": "9806df17-06ee-4399-f156-4491b6711a67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "bIid2lCsgwlm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKDK7wPxgXRh",
        "outputId": "5d9f676c-e0bf-4f19-87ac-f61246c5458f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Welcome',\n",
              " 'to',\n",
              " 'NLP',\n",
              " 'learning',\n",
              " 'We',\n",
              " 'have',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'ground',\n",
              " 'to',\n",
              " 'cover',\n",
              " 'Years',\n",
              " 'of',\n",
              " 'work',\n",
              " 'Let',\n",
              " \"'s\",\n",
              " 'catch',\n",
              " 'up',\n",
              " 'in',\n",
              " 'a',\n",
              " 'few',\n",
              " 'months']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in docs:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmEYoVIPhLMd",
        "outputId": "462a4b77-024f-43bb-82c2-6428070d92aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Welcome', 'to', 'NLP', 'learning', 'We', 'have', 'a', 'lot', 'of', 'ground', 'to', 'cover', 'Years', 'of', 'work', 'Let', \"'s\", 'catch', 'up', 'in', 'a', 'few', 'months']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "PAzxih9ThR3Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CNliddyhVl4",
        "outputId": "847ca941-dd7f-4661-cf1f-c463d351a25e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Welcome',\n",
              " 'to',\n",
              " 'NLP',\n",
              " 'learning',\n",
              " 'We',\n",
              " 'have',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'ground',\n",
              " 'to',\n",
              " 'cover',\n",
              " 'Years',\n",
              " 'of',\n",
              " 'work',\n",
              " 'Let',\n",
              " \"'\",\n",
              " 's',\n",
              " 'catch',\n",
              " 'up',\n",
              " 'in',\n",
              " 'a',\n",
              " 'few',\n",
              " 'months']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "LUVngmzYhbLS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = TreebankWordTokenizer()\n",
        "token.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz8H8Uaehie_",
        "outputId": "67368c9c-c0da-4ed4-947b-dc26c561bdbe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Welcome',\n",
              " 'to',\n",
              " 'NLP',\n",
              " 'learning',\n",
              " 'We',\n",
              " 'have',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'ground',\n",
              " 'to',\n",
              " 'cover',\n",
              " 'Years',\n",
              " 'of',\n",
              " 'work',\n",
              " 'Let',\n",
              " \"'s\",\n",
              " 'catch',\n",
              " 'up',\n",
              " 'in',\n",
              " 'a',\n",
              " 'few',\n",
              " 'months']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['eating','eat','eaten','going','gone','programming', 'programs','writing','writes','written', 'history', 'fairly','sportingly']"
      ],
      "metadata": {
        "id": "IG3znS_SjDO4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Porter Stemmer"
      ],
      "metadata": {
        "id": "PQxNOZAnjQfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "D7BP7ML9jSMO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "dMJhAhsbjW4m"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+ \" ---> \"+ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5tutwqWjZOX",
        "outputId": "ce4df373-71db-43c1-fcf0-40c984635eb6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eat ---> eat\n",
            "eaten ---> eaten\n",
            "going ---> go\n",
            "gone ---> gone\n",
            "programming ---> program\n",
            "programs ---> program\n",
            "writing ---> write\n",
            "writes ---> write\n",
            "written ---> written\n",
            "history ---> histori\n",
            "fairly ---> fairli\n",
            "sportingly ---> sportingli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regexp Stemmer"
      ],
      "metadata": {
        "id": "0tEZ18Qgj6wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "0fMXKyMDj-N-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regexst = RegexpStemmer('ing$|e$|ed$|s$|en$', min=4)"
      ],
      "metadata": {
        "id": "_3BmeDcukCp_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regexst.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZbBr6nygkRxL",
        "outputId": "f5584f8a-b9bc-4684-e117-6b46bb35e829"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Snowball Stemmer"
      ],
      "metadata": {
        "id": "mM85uV2vkrLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "dtTWNnnEkvB1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbs = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "sMU-e3rnkyKS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\" --> \"+ sbs.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0QGfackk7-K",
        "outputId": "0e4662af-122c-4296-cad3-f6cefbe6cd02"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating --> eat\n",
            "eat --> eat\n",
            "eaten --> eaten\n",
            "going --> go\n",
            "gone --> gone\n",
            "programming --> program\n",
            "programs --> program\n",
            "writing --> write\n",
            "writes --> write\n",
            "written --> written\n",
            "history --> histori\n",
            "fairly --> fair\n",
            "sportingly --> sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "lk0AJ1cjlpjW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXX67yiqlrAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}